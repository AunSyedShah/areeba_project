{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Step 1: Create SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"CSV to Parquet\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Step 2: Read CSV file\n",
        "# Use the path from the kagglehub download\n",
        "csv_file_path = \"/content/drive/MyDrive/climate_dataset/GlobalLandTemperaturesByCity.csv\"\n",
        "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
        "\n",
        "# Step 3: Repartition the DataFrame (e.g., into 4 partitions)\n",
        "df = df.repartition(4)\n",
        "\n",
        "# Step 4: Write to Parquet (as a single file but partitioned into chunks)\n",
        "df.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .parquet(\"output_folder\")"
      ],
      "metadata": {
        "id": "hjKrjm0YnHmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Step 1: Create SparkSession (if not already created)\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Read Parquet File\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Step 2: Read the Parquet data\n",
        "df_parquet = spark.read.parquet(\"output_folder\")\n",
        "\n",
        "# Step 3: Show the DataFrame\n",
        "df_parquet.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQFigwfdo8Kj",
        "outputId": "751c89f9-1b50-49bf-d7a7-2b2e8e1947e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------------+-----------------------------+-----------------+--------------+--------+---------+\n",
            "|        dt|AverageTemperature|AverageTemperatureUncertainty|             City|       Country|Latitude|Longitude|\n",
            "+----------+------------------+-----------------------------+-----------------+--------------+--------+---------+\n",
            "|1848-09-01|            27.435|                        1.042|          Barisal|    Bangladesh|  23.31N|   90.00E|\n",
            "|1838-10-01|              NULL|                         NULL|         Cikampek|     Indonesia|   7.23S|  107.84E|\n",
            "|1994-03-01|            19.507|                         1.12|         Buraydah|  Saudi Arabia|  26.52N|   44.78E|\n",
            "|1888-11-01|            25.916|          0.46299999999999997|         Carúpano|     Venezuela|  10.45N|   63.00W|\n",
            "|1857-05-01|            26.017|                        1.193|          Cotonou|         Benin|   7.23N|    2.43E|\n",
            "|1890-06-01|            23.257|                        1.141|       Cape Coast|         Ghana|   5.63N|    1.61W|\n",
            "|1950-02-01|            24.426|                        0.618|        Barcelona|     Venezuela|  10.45N|   64.64W|\n",
            "|1833-02-01|             5.694|                        2.302|Alcalá De Henares|         Spain|  40.99N|    4.26W|\n",
            "|1922-09-01|            27.959|                        0.239|    Bhairab Bazar|    Bangladesh|  24.92N|   90.44E|\n",
            "|1969-11-01|            24.747|                        0.309|            Akola|         India|  20.09N|   76.78E|\n",
            "|1931-08-01|            29.432|                        0.191|        Begusarai|         India|  24.92N|   86.90E|\n",
            "|1930-12-01|            -3.931|                         0.68| Colorado Springs| United States|  39.38N|  104.05W|\n",
            "|1896-02-01|            23.605|           0.7829999999999999|          Bamenda|      Cameroon|   5.63N|    9.69E|\n",
            "|1909-12-01|             6.063|                        0.523|         Bordeaux|        France|  44.20N|    0.00W|\n",
            "|1983-08-01|             22.84|                        0.445|        Algeciras|         Spain|  36.17N|    5.97W|\n",
            "|1934-08-01|            28.884|                        0.133|          Bangaon|         India|  23.31N|   88.25E|\n",
            "|1798-08-01|            16.169|                        6.449|         Coventry|United Kingdom|  52.24N|    2.63W|\n",
            "|1884-11-01|            15.288|           0.9420000000000001|        Algeciras|         Spain|  36.17N|    5.97W|\n",
            "|1838-03-01|             8.064|                         2.31|           Bilbao|         Spain|  42.59N|    2.18W|\n",
            "|1986-03-01|             4.663|                        0.231|          Brescia|         Italy|  45.81N|   10.38E|\n",
            "+----------+------------------+-----------------------------+-----------------+--------------+--------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sort by AverageTemperature ignore where NULL\n",
        "df_parquet = df_parquet.sort(\"AverageTemperature\")\n",
        "df_parquet.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vemn7AcY7FKJ",
        "outputId": "8899ed49-5b39-4a63-dff7-38418a0c2621"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------------+-----------------------------+--------------+-------------+--------+---------+\n",
            "|        dt|AverageTemperature|AverageTemperatureUncertainty|          City|      Country|Latitude|Longitude|\n",
            "+----------+------------------+-----------------------------+--------------+-------------+--------+---------+\n",
            "|1778-07-01|              NULL|                         NULL|       Detroit|United States|  42.59N|   82.91W|\n",
            "|1814-05-01|              NULL|                         NULL|         Bidar|        India|  18.48N|   77.75E|\n",
            "|1750-11-01|              NULL|                         NULL|        Almere|  Netherlands|  52.24N|    5.26E|\n",
            "|1755-07-01|              NULL|                         NULL|    Des Moines|United States|  40.99N|   93.73W|\n",
            "|1890-07-01|              NULL|                         NULL|         Cusco|         Peru|  13.66S|   71.83W|\n",
            "|1747-06-01|              NULL|                         NULL|   Chattanooga|United States|  34.56N|   85.62W|\n",
            "|1824-08-01|              NULL|                         NULL|      Edmonton|       Canada|  53.84N|  113.18W|\n",
            "|1843-07-01|              NULL|                         NULL|      Cibinong|    Indonesia|   5.63S|  106.55E|\n",
            "|1847-11-01|              NULL|                         NULL|Angra Dos Reis|       Brazil|  23.31S|   44.56W|\n",
            "|1808-05-01|              NULL|                         NULL|   Baton Rouge|United States|  29.74N|   90.46W|\n",
            "|1841-06-01|              NULL|                         NULL|   Banjarmasin|    Indonesia|   4.02S|  114.91E|\n",
            "|1863-07-01|              NULL|                         NULL|     Chandausi|        India|  28.13N|   79.09E|\n",
            "|1833-06-01|              NULL|                         NULL|        Cisaat|    Indonesia|   7.23S|  106.22E|\n",
            "|1874-08-01|              NULL|                         NULL|     Antsirabe|   Madagascar|  20.09S|   47.77E|\n",
            "|1798-11-01|              NULL|                         NULL|        Denver|United States|  39.38N|  104.05W|\n",
            "|1849-06-01|              NULL|                         NULL|     Concordia|    Argentina|  31.35S|   58.43W|\n",
            "|1827-07-01|              NULL|                         NULL|     Cileungsi|    Indonesia|   5.63S|  106.55E|\n",
            "|1836-03-01|              NULL|                         NULL|       Cirebon|    Indonesia|   7.23S|  107.84E|\n",
            "|1875-04-01|              NULL|                         NULL|         Beira|   Mozambique|  20.09S|   34.12E|\n",
            "|1825-07-01|              NULL|                         NULL|        Anyang|        China|  36.17N|  113.37E|\n",
            "+----------+------------------+-----------------------------+--------------+-------------+--------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove all the null values\n",
        "null_df = df_parquet.na.drop()\n",
        "null_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7ZyJJRg-XUE",
        "outputId": "1713301e-c280-4a6f-d1db-2adca493b243"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------------+-----------------------------+----------+-------+--------+---------+\n",
            "|        dt| AverageTemperature|AverageTemperatureUncertainty|      City|Country|Latitude|Longitude|\n",
            "+----------+-------------------+-----------------------------+----------+-------+--------+---------+\n",
            "|1979-02-01| -42.70399999999999|                        0.972|   Norilsk| Russia|  69.92N|   88.83E|\n",
            "|1893-01-01|-41.101000000000006|                          1.5|     Kyzyl| Russia|  52.24N|   94.60E|\n",
            "|1966-02-01|-39.919000000000004|                        0.968|   Norilsk| Russia|  69.92N|   88.83E|\n",
            "|1974-01-01|            -39.683|                        1.298|   Norilsk| Russia|  69.92N|   88.83E|\n",
            "|1979-01-01|            -39.403|                        0.387|   Norilsk| Russia|  69.92N|   88.83E|\n",
            "|1919-01-01|-39.038000000000004|                          1.5|     Kyzyl| Russia|  52.24N|   94.60E|\n",
            "|1872-01-01|            -38.951|                        6.653|     Kyzyl| Russia|  52.24N|   94.60E|\n",
            "|1969-01-01|            -38.906|           0.7609999999999999|     Kyzyl| Russia|  52.24N|   94.60E|\n",
            "|1969-02-01|            -38.784|           0.9740000000000001|   Norilsk| Russia|  69.92N|   88.83E|\n",
            "|1922-02-01|            -38.715|                        2.144|   Norilsk| Russia|  69.92N|   88.83E|\n",
            "|1900-01-01|-38.693000000000005|                         2.19|     Kyzyl| Russia|  52.24N|   94.60E|\n",
            "|2000-12-01|-38.446999999999996|                        1.138|   Norilsk| Russia|  69.92N|   88.83E|\n",
            "|1862-01-01|            -38.396|            5.417999999999999|     Kyzyl| Russia|  52.24N|   94.60E|\n",
            "|2001-01-01|            -38.389|           1.2790000000000001|   Norilsk| Russia|  69.92N|   88.83E|\n",
            "|2007-02-01|            -38.344|                        2.174|   Norilsk| Russia|  69.92N|   88.83E|\n",
            "|1895-02-01|            -38.085|                        3.057|   Norilsk| Russia|  69.92N|   88.83E|\n",
            "|1960-02-01|            -37.869|                        1.719|   Norilsk| Russia|  69.92N|   88.83E|\n",
            "|1893-01-01|            -37.713|           4.2010000000000005|Ust Ilimsk| Russia|  58.66N|  101.54E|\n",
            "|1922-01-01|            -37.625|                        1.945|     Kyzyl| Russia|  52.24N|   94.60E|\n",
            "|1885-01-01|            -37.488|           2.5340000000000003|   Norilsk| Russia|  69.92N|   88.83E|\n",
            "+----------+-------------------+-----------------------------+----------+-------+--------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get total count\n",
        "null_df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ocC-AsM-wsa",
        "outputId": "dbe39661-50d6-426f-ce6f-fc089a3796e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8235082"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "null_df.sort(\"AverageTemperature\").show(15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPuyLYos-0Ix",
        "outputId": "530574f8-fac5-4287-a639-b1ea32ffa27a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------------+-----------------------------+-------+-------+--------+---------+\n",
            "|        dt| AverageTemperature|AverageTemperatureUncertainty|   City|Country|Latitude|Longitude|\n",
            "+----------+-------------------+-----------------------------+-------+-------+--------+---------+\n",
            "|1979-02-01| -42.70399999999999|                        0.972|Norilsk| Russia|  69.92N|   88.83E|\n",
            "|1893-01-01|-41.101000000000006|                          1.5|  Kyzyl| Russia|  52.24N|   94.60E|\n",
            "|1966-02-01|-39.919000000000004|                        0.968|Norilsk| Russia|  69.92N|   88.83E|\n",
            "|1974-01-01|            -39.683|                        1.298|Norilsk| Russia|  69.92N|   88.83E|\n",
            "|1979-01-01|            -39.403|                        0.387|Norilsk| Russia|  69.92N|   88.83E|\n",
            "|1919-01-01|-39.038000000000004|                          1.5|  Kyzyl| Russia|  52.24N|   94.60E|\n",
            "|1872-01-01|            -38.951|                        6.653|  Kyzyl| Russia|  52.24N|   94.60E|\n",
            "|1969-01-01|            -38.906|           0.7609999999999999|  Kyzyl| Russia|  52.24N|   94.60E|\n",
            "|1969-02-01|            -38.784|           0.9740000000000001|Norilsk| Russia|  69.92N|   88.83E|\n",
            "|1922-02-01|            -38.715|                        2.144|Norilsk| Russia|  69.92N|   88.83E|\n",
            "|1900-01-01|-38.693000000000005|                         2.19|  Kyzyl| Russia|  52.24N|   94.60E|\n",
            "|2000-12-01|-38.446999999999996|                        1.138|Norilsk| Russia|  69.92N|   88.83E|\n",
            "|1862-01-01|            -38.396|            5.417999999999999|  Kyzyl| Russia|  52.24N|   94.60E|\n",
            "|2001-01-01|            -38.389|           1.2790000000000001|Norilsk| Russia|  69.92N|   88.83E|\n",
            "|2007-02-01|            -38.344|                        2.174|Norilsk| Russia|  69.92N|   88.83E|\n",
            "+----------+-------------------+-----------------------------+-------+-------+--------+---------+\n",
            "only showing top 15 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Show extremely low or high temperatures\n",
        "null_df.filter((col(\"AverageTemperature\") < -50) | (col(\"AverageTemperature\") > 50)).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dG0pxSJc_UAA",
        "outputId": "5054e52e-9fd5-4113-8304-87348af17104"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------------+-----------------------------+----+-------+--------+---------+\n",
            "| dt|AverageTemperature|AverageTemperatureUncertainty|City|Country|Latitude|Longitude|\n",
            "+---+------------------+-----------------------------+----+-------+--------+---------+\n",
            "+---+------------------+-----------------------------+----+-------+--------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import mean, stddev, col\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# -----------------------------------------\n",
        "# STEP 1: Filter for Pakistan\n",
        "# -----------------------------------------\n",
        "pakistan_df = null_df.filter(col(\"Country\") == \"Pakistan\")\n",
        "\n",
        "# -----------------------------------------\n",
        "# STEP 2: Calculate Mean and Std Deviation (for Pakistan only)\n",
        "# -----------------------------------------\n",
        "stats = pakistan_df.select(\n",
        "    mean(\"AverageTemperature\").alias(\"mean_temp\"),\n",
        "    stddev(\"AverageTemperature\").alias(\"stddev_temp\")\n",
        ").first()\n",
        "\n",
        "mean_temp = stats[\"mean_temp\"]\n",
        "stddev_temp = stats[\"stddev_temp\"]\n",
        "\n",
        "print(f\"[PAKISTAN] Mean Temp: {mean_temp:.2f}, Std Dev: {stddev_temp:.2f}\")\n",
        "\n",
        "# -----------------------------------------\n",
        "# STEP 3: Add z_score Column\n",
        "# -----------------------------------------\n",
        "pakistan_df_with_z = pakistan_df.withColumn(\n",
        "    \"z_score\",\n",
        "    (col(\"AverageTemperature\") - mean_temp) / stddev_temp\n",
        ")\n",
        "\n",
        "# -----------------------------------------\n",
        "# STEP 4: Filter Anomalies\n",
        "# -----------------------------------------\n",
        "anomalies_df = pakistan_df_with_z.filter((col(\"z_score\") > 3) | (col(\"z_score\") < -3))\n",
        "\n",
        "# -----------------------------------------\n",
        "# STEP 5: Convert to Pandas (safe limit)\n",
        "# -----------------------------------------\n",
        "anomalies_pd = anomalies_df.select(\n",
        "    \"dt\", \"AverageTemperature\", \"z_score\", \"City\", \"Country\"\n",
        ").orderBy(col(\"z_score\").desc()).limit(500).toPandas()\n",
        "\n",
        "anomalies_pd[\"dt\"] = pd.to_datetime(anomalies_pd[\"dt\"])\n",
        "\n",
        "# -----------------------------------------\n",
        "# STEP 6: Plot Anomalies for Pakistan\n",
        "# -----------------------------------------\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.scatter(anomalies_pd[\"dt\"], anomalies_pd[\"AverageTemperature\"], color='red', label='Anomalies')\n",
        "plt.axhline(mean_temp, color='green', linestyle='--', label='Mean Temperature (Pakistan)')\n",
        "\n",
        "plt.title(\"Temperature Anomalies in Pakistan (Z-score > 3 or < -3)\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Average Temperature (°C)\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "D2FZ_zSKIYxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, mean as _mean, stddev as _stddev\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Temperature Anomalies\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Load your dataset (replace with your path if different)\n",
        "df = spark.read.parquet(\"output_folder\")\n",
        "\n",
        "# Get distinct countries for dropdown\n",
        "countries = [row['Country'] for row in df.select(\"Country\").distinct().collect()]\n",
        "\n",
        "# Function to calculate and plot anomalies\n",
        "def plot_anomalies(country):\n",
        "    # Filter for selected country\n",
        "    country_df = df.filter(col(\"Country\") == country)\n",
        "\n",
        "    # Calculate mean and stddev\n",
        "    stats = country_df.select(\n",
        "        _mean(\"AverageTemperature\").alias(\"mean\"),\n",
        "        _stddev(\"AverageTemperature\").alias(\"std\")\n",
        "    ).collect()[0]\n",
        "    mean_val, std_val = stats[\"mean\"], stats[\"std\"]\n",
        "\n",
        "    # Calculate anomaly (z-score)\n",
        "    anomalies_df = country_df.withColumn(\"Anomaly\", (col(\"AverageTemperature\") - mean_val) / std_val)\n",
        "\n",
        "    # Convert to Pandas\n",
        "    pdf = anomalies_df.select(\"dt\", \"Anomaly\").orderBy(\"dt\").toPandas()\n",
        "    pdf['dt'] = pd.to_datetime(pdf['dt'])\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(pdf['dt'], pdf['Anomaly'], label='Anomaly (z-score)', color='purple')\n",
        "    plt.axhline(0, color='black', linestyle='--', linewidth=1)\n",
        "    plt.title(f'Temperature Anomalies for {country}')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Anomaly (Z-Score)')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save plot to image\n",
        "    plot_path = \"/tmp/anomaly_plot.png\"\n",
        "    plt.savefig(plot_path)\n",
        "    plt.close()\n",
        "    return plot_path\n",
        "\n",
        "# Gradio UI\n",
        "interface = gr.Interface(\n",
        "    fn=plot_anomalies,\n",
        "    inputs=gr.Dropdown(choices=countries, label=\"Select Country\"),\n",
        "    outputs=gr.Image(type=\"filepath\"),\n",
        "    title=\"Temperature Anomaly Viewer\",\n",
        "    description=\"Select a country to view its temperature anomalies over time (Z-score)\"\n",
        ")\n",
        "\n",
        "# Launch\n",
        "interface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "fjwqkd7VIzRh",
        "outputId": "56761fe1-c6cc-4856-81af-a85686eac35e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://54c2bd6d42ccc5d421.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://54c2bd6d42ccc5d421.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "from pyspark.sql.functions import month, year, avg\n",
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import gradio as gr\n",
        "\n",
        "# Step 0: Spark session (only needed if not already created)\n",
        "spark = SparkSession.builder.appName(\"ClimatePatternApp\").getOrCreate()\n",
        "\n",
        "# Step 1: Load and clean data (replace path as needed)\n",
        "df = spark.read.parquet(\"output_folder\")  # Adjust file path\n",
        "df = df.na.drop()\n",
        "\n",
        "# Step 2: Add month and year\n",
        "df = df.withColumn(\"month\", month(\"dt\")).withColumn(\"year\", year(\"dt\"))\n",
        "\n",
        "# Step 3: Monthly and Yearly Patterns\n",
        "monthly_pattern_df = df.groupBy(\"Country\", \"month\") \\\n",
        "                       .agg(avg(\"AverageTemperature\").alias(\"AvgTemp\")) \\\n",
        "                       .orderBy(\"Country\", \"month\")\n",
        "\n",
        "yearly_trend_df = df.groupBy(\"Country\", \"year\") \\\n",
        "                    .agg(avg(\"AverageTemperature\").alias(\"YearlyAvgTemp\")) \\\n",
        "                    .orderBy(\"Country\", \"year\")\n",
        "\n",
        "# Step 4: Anomaly Detection (IQR method per country)\n",
        "pdf = df.select(\"dt\", \"AverageTemperature\", \"Country\").toPandas()\n",
        "anomalies = []\n",
        "\n",
        "for country in pdf[\"Country\"].unique():\n",
        "    subset = pdf[pdf[\"Country\"] == country]\n",
        "    Q1 = subset[\"AverageTemperature\"].quantile(0.25)\n",
        "    Q3 = subset[\"AverageTemperature\"].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower = Q1 - 1.5 * IQR\n",
        "    upper = Q3 + 1.5 * IQR\n",
        "    anom = subset[(subset[\"AverageTemperature\"] < lower) | (subset[\"AverageTemperature\"] > upper)]\n",
        "    anomalies.append(anom)\n",
        "\n",
        "anomalies_pd = pd.concat(anomalies)\n",
        "\n",
        "# Step 5: Convert patterns to Pandas\n",
        "monthly_pattern_pd = monthly_pattern_df.toPandas()\n",
        "yearly_trend_pd = yearly_trend_df.toPandas()\n",
        "\n",
        "# Step 6: Gradio Function\n",
        "def show_patterns(country):\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(21, 5))\n",
        "\n",
        "    # Monthly Pattern\n",
        "    monthly = monthly_pattern_pd[monthly_pattern_pd[\"Country\"] == country]\n",
        "    axes[0].plot(monthly[\"month\"], monthly[\"AvgTemp\"], marker='o')\n",
        "    axes[0].set_title(f\"Monthly Climate Pattern - {country}\")\n",
        "    axes[0].set_xlabel(\"Month\")\n",
        "    axes[0].set_ylabel(\"Avg Temp (°C)\")\n",
        "    axes[0].grid(True)\n",
        "\n",
        "    # Yearly Trend\n",
        "    yearly = yearly_trend_pd[yearly_trend_pd[\"Country\"] == country]\n",
        "    axes[1].plot(yearly[\"year\"], yearly[\"YearlyAvgTemp\"], marker='x', color='orange')\n",
        "    axes[1].set_title(f\"Yearly Temp Trend - {country}\")\n",
        "    axes[1].set_xlabel(\"Year\")\n",
        "    axes[1].set_ylabel(\"Avg Temp (°C)\")\n",
        "    axes[1].grid(True)\n",
        "\n",
        "    # Anomalies\n",
        "    anom = anomalies_pd[anomalies_pd[\"Country\"] == country]\n",
        "    axes[2].plot(anom[\"dt\"], anom[\"AverageTemperature\"], 'ro', markersize=4)\n",
        "    axes[2].set_title(f\"Detected Anomalies - {country}\")\n",
        "    axes[2].set_xlabel(\"Date\")\n",
        "    axes[2].set_ylabel(\"Anomalous Temp (°C)\")\n",
        "    axes[2].grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "# Step 7: Launch Gradio App\n",
        "country_list = sorted(monthly_pattern_pd[\"Country\"].unique())\n",
        "\n",
        "gr.Interface(fn=show_patterns,\n",
        "             inputs=gr.Dropdown(choices=country_list, label=\"Select Country\"),\n",
        "             outputs=gr.Plot(label=\"Climate Insights\"),\n",
        "             title=\"🌍 Climate Anomalies & Patterns Visualizer\",\n",
        "             description=\"Select a country to view monthly weather pattern, yearly trends, and temperature anomalies.\").launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EmLtZ9vyKTC_",
        "outputId": "c3e65209-23c6-43da-e75f-93c5e0027931"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o528.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 119.0 failed 1 times, most recent failure: Lost task 0.0 in stage 119.0 (TID 183) (fbd16ade332a executor driver): java.lang.OutOfMemoryError: Java heap space\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4148)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4145)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2322458645.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Step 4: Anomaly Detection (IQR method per country)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"AverageTemperature\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Country\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0manomalies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/pandas/conversion.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;31m# Below is toPandas without Arrow optimization.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             pdf = pd.DataFrame.from_records(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \"\"\"\n\u001b[1;32m   1260\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1261\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1262\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o528.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 119.0 failed 1 times, most recent failure: Lost task 0.0 in stage 119.0 (TID 183) (fbd16ade332a executor driver): java.lang.OutOfMemoryError: Java heap space\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4148)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4145)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7hREeOqfMIYz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}