{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ›ï¸ Dashboard Launch Helper loaded!\n",
      "ğŸ“– Usage: launch_dashboard() or launch_dashboard(share=True, port=7860)\n",
      "âš ï¸ NOTE: Run cells 3-9 first to create the dashboard, then use launch_dashboard()\n"
     ]
    }
   ],
   "source": [
    "# ğŸš€ Dashboard Launch Helper\n",
    "\n",
    "def launch_dashboard(share=False, port=7860):\n",
    "    \"\"\"\n",
    "    Launch the enhanced dashboard\n",
    "    \n",
    "    Args:\n",
    "        share (bool): Whether to create a public link\n",
    "        port (int): Port to run the server on\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if 'enhanced_dashboard' not in globals():\n",
    "            print(\"âŒ Enhanced dashboard not found. Please run the dashboard creation cell first.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"ğŸš€ Launching Enhanced Climate Dashboard on port {port}...\")\n",
    "        print(\"ğŸ“Š Dashboard Features:\")\n",
    "        print(\"   - Country Analysis with PySpark backend\")\n",
    "        print(\"   - Global Comparisons\")\n",
    "        print(\"   - AI Temperature Predictions (demo)\")\n",
    "        print(\"   - System Monitoring\")\n",
    "        print(\"   - Interactive Visualizations\")\n",
    "        print()\n",
    "        \n",
    "        # Launch with threading to avoid blocking\n",
    "        import threading\n",
    "        \n",
    "        def launch_in_background():\n",
    "            enhanced_dashboard.launch(\n",
    "                share=share,\n",
    "                server_name=\"0.0.0.0\",\n",
    "                server_port=port,\n",
    "                show_error=True,\n",
    "                quiet=False,\n",
    "                prevent_thread_lock=True\n",
    "            )\n",
    "        \n",
    "        thread = threading.Thread(target=launch_in_background, daemon=True)\n",
    "        thread.start()\n",
    "        \n",
    "        print(f\"ğŸŒ Dashboard should be available at: http://localhost:{port}\")\n",
    "        if share:\n",
    "            print(\"ğŸ”— Public link will be generated...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error launching dashboard: {e}\")\n",
    "\n",
    "print(\"ğŸ›ï¸ Dashboard Launch Helper loaded!\")\n",
    "print(\"ğŸ“– Usage: launch_dashboard() or launch_dashboard(share=True, port=7860)\")\n",
    "print(\"âš ï¸ NOTE: Run cells 3-9 first to create the dashboard, then use launch_dashboard()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Climate Data Analysis - Optimized\n",
    "\n",
    "This notebook demonstrates optimized PySpark operations for climate data analysis including:\n",
    "- Efficient Spark configuration\n",
    "- Data loading and preprocessing\n",
    "- Performance-optimized transformations\n",
    "- Interactive visualizations with Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/areeba_project/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/08/16 07:30:31 WARN Utils: Your hostname, codespaces-01cd44, resolves to a loopback address: 127.0.0.1; using 10.0.1.114 instead (on interface eth0)\n",
      "25/08/16 07:30:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/08/16 07:30:31 WARN Utils: Your hostname, codespaces-01cd44, resolves to a loopback address: 127.0.0.1; using 10.0.1.114 instead (on interface eth0)\n",
      "25/08/16 07:30:31 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/16 07:30:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/08/16 07:30:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 4.0.0\n",
      "Spark UI available at: http://34fd580d-f292-40bc-8d12-3ae5cb26456d.internal.cloudapp.net:4040\n"
     ]
    }
   ],
   "source": [
    "# Optimized imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, mean, stddev, month, year, avg, \n",
    "    when, isnan, isnull, broadcast, \n",
    "    percentile_approx, desc, asc, count,\n",
    "    min as spark_min, max as spark_max\n",
    ")\n",
    "from pyspark.sql.types import DoubleType\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gradio as gr\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Also import plotly for advanced visualizations\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "\n",
    "# Optimized Spark Configuration\n",
    "def create_optimized_spark_session():\n",
    "    \"\"\"Create an optimized Spark session for climate data analysis\"\"\"\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"OptimizedClimateAnalysis\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.repl.eagerEval.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.repl.eagerEval.maxNumRows\", \"20\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # Set log level to reduce verbosity\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    return spark\n",
    "\n",
    "# Initialize optimized Spark session\n",
    "spark = create_optimized_spark_session()\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Spark UI available at: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded existing Parquet data\n",
      "ğŸ”„ Adding missing derived columns (month, year)...\n",
      "âœ… Added derived columns and cached dataset\n",
      "\n",
      "ğŸ“Š Dataset Overview:\n",
      "root\n",
      " |-- dt: date (nullable = true)\n",
      " |-- AverageTemperature: double (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: double (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- temp_celsius: double (nullable = true)\n",
      "\n",
      "âœ… Added derived columns and cached dataset\n",
      "\n",
      "ğŸ“Š Dataset Overview:\n",
      "root\n",
      " |-- dt: date (nullable = true)\n",
      " |-- AverageTemperature: double (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: double (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- temp_celsius: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:=======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|AverageTemperature|\n",
      "+-------+------------------+\n",
      "|  count|           8235082|\n",
      "|   mean|16.727432636249816|\n",
      "| stddev|10.353442482534478|\n",
      "|    min|-42.70399999999999|\n",
      "|    max|            39.651|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Optimized Data Loading and Preprocessing\n",
    "\n",
    "def load_and_preprocess_data(file_path=\"GlobalLandTemperaturesByCity.csv\", output_path=\"output_folder\"):\n",
    "    \"\"\"\n",
    "    Load CSV data and convert to optimized Parquet format with preprocessing\n",
    "    \"\"\"\n",
    "    print(\"ğŸ“Š Loading and preprocessing climate data...\")\n",
    "    \n",
    "    # Read CSV with optimized schema inference\n",
    "    df = spark.read.option(\"header\", \"true\") \\\n",
    "                  .option(\"inferSchema\", \"true\") \\\n",
    "                  .option(\"timestampFormat\", \"yyyy-MM-dd\") \\\n",
    "                  .csv(file_path)\n",
    "    \n",
    "    print(f\"ğŸ“ˆ Original dataset: {df.count():,} rows, {len(df.columns)} columns\")\n",
    "    \n",
    "    # Data quality checks and cleaning\n",
    "    df_clean = df.filter(\n",
    "        col(\"AverageTemperature\").isNotNull() & \n",
    "        col(\"Country\").isNotNull() & \n",
    "        col(\"City\").isNotNull() &\n",
    "        col(\"dt\").isNotNull()\n",
    "    ).filter(\n",
    "        # Remove extreme outliers (temperatures beyond physical limits)\n",
    "        (col(\"AverageTemperature\") >= -80) & \n",
    "        (col(\"AverageTemperature\") <= 60)\n",
    "    )\n",
    "    \n",
    "    # Add derived columns for better partitioning and analysis\n",
    "    df_enhanced = df_clean.withColumn(\"year\", year(\"dt\")) \\\n",
    "                         .withColumn(\"month\", month(\"dt\")) \\\n",
    "                         .withColumn(\"temp_celsius\", col(\"AverageTemperature\").cast(DoubleType()))\n",
    "    \n",
    "    # Optimal partitioning strategy - partition by year for time-series analysis\n",
    "    df_partitioned = df_enhanced.repartition(8, \"year\")\n",
    "    \n",
    "    # Cache the cleaned dataset for multiple operations\n",
    "    df_partitioned.cache()\n",
    "    \n",
    "    print(f\"âœ… Cleaned dataset: {df_partitioned.count():,} rows\")\n",
    "    \n",
    "    # Write to optimized Parquet format\n",
    "    df_partitioned.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"compression\", \"snappy\") \\\n",
    "        .partitionBy(\"year\") \\\n",
    "        .parquet(output_path)\n",
    "    \n",
    "    print(f\"ğŸ’¾ Data saved to {output_path} in Parquet format\")\n",
    "    return df_partitioned\n",
    "\n",
    "# Load and preprocess data\n",
    "try:\n",
    "    # Try to load existing parquet data first\n",
    "    climate_df_raw = spark.read.parquet(\"output_folder\")\n",
    "    print(\"âœ… Loaded existing Parquet data\")\n",
    "    \n",
    "    # Check if derived columns exist, if not add them\n",
    "    if \"month\" not in climate_df_raw.columns or \"year\" not in climate_df_raw.columns:\n",
    "        print(\"ğŸ”„ Adding missing derived columns (month, year)...\")\n",
    "        climate_df = climate_df_raw.withColumn(\"year\", year(\"dt\")) \\\n",
    "                                   .withColumn(\"month\", month(\"dt\")) \\\n",
    "                                   .withColumn(\"temp_celsius\", col(\"AverageTemperature\").cast(DoubleType()))\n",
    "        \n",
    "        # Cache the enhanced dataset\n",
    "        climate_df.cache()\n",
    "        print(\"âœ… Added derived columns and cached dataset\")\n",
    "    else:\n",
    "        climate_df = climate_df_raw\n",
    "        climate_df.cache()\n",
    "        print(\"âœ… Dataset already has derived columns\")\n",
    "        \n",
    "except:\n",
    "    # If parquet doesn't exist, process from CSV\n",
    "    print(\"ğŸ“‚ Parquet file not found, processing from CSV...\")\n",
    "    climate_df = load_and_preprocess_data()\n",
    "\n",
    "# Show basic statistics\n",
    "print(\"\\nğŸ“Š Dataset Overview:\")\n",
    "climate_df.printSchema()\n",
    "climate_df.describe(\"AverageTemperature\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Detecting temperature anomalies for Pakistan...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Temperature stats for Pakistan:\n",
      "   Mean: 23.97Â°C, Std Dev: 7.91Â°C\n",
      "ğŸš¨ Found 42 temperature anomalies (|z-score| > 2.5)\n",
      "\n",
      "ğŸ”¥ Top Temperature Anomalies:\n",
      "ğŸš¨ Found 42 temperature anomalies (|z-score| > 2.5)\n",
      "\n",
      "ğŸ”¥ Top Temperature Anomalies:\n",
      "+----------+--------+------------------+-------------------+\n",
      "|        dt|    City|AverageTemperature|            z_score|\n",
      "+----------+--------+------------------+-------------------+\n",
      "|1964-01-01|  Mardan|              4.16|  -2.50439790547346|\n",
      "|1964-01-01|Mingaora|              4.16|  -2.50439790547346|\n",
      "|1964-01-01|     Wah|              4.16|  -2.50439790547346|\n",
      "|1964-01-01|Peshawar|              4.16|  -2.50439790547346|\n",
      "|1893-02-01|  Mardan|4.0360000000000005| -2.520071959125641|\n",
      "|1893-02-01|Peshawar|4.0360000000000005| -2.520071959125641|\n",
      "|1893-02-01|Mingaora|4.0360000000000005| -2.520071959125641|\n",
      "|1893-02-01|     Wah|4.0360000000000005| -2.520071959125641|\n",
      "|1895-01-01|  Mardan|             4.011|-2.5232320505877746|\n",
      "|1895-01-01|     Wah|             4.011|-2.5232320505877746|\n",
      "+----------+--------+------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "+----------+--------+------------------+-------------------+\n",
      "|        dt|    City|AverageTemperature|            z_score|\n",
      "+----------+--------+------------------+-------------------+\n",
      "|1964-01-01|  Mardan|              4.16|  -2.50439790547346|\n",
      "|1964-01-01|Mingaora|              4.16|  -2.50439790547346|\n",
      "|1964-01-01|     Wah|              4.16|  -2.50439790547346|\n",
      "|1964-01-01|Peshawar|              4.16|  -2.50439790547346|\n",
      "|1893-02-01|  Mardan|4.0360000000000005| -2.520071959125641|\n",
      "|1893-02-01|Peshawar|4.0360000000000005| -2.520071959125641|\n",
      "|1893-02-01|Mingaora|4.0360000000000005| -2.520071959125641|\n",
      "|1893-02-01|     Wah|4.0360000000000005| -2.520071959125641|\n",
      "|1895-01-01|  Mardan|             4.011|-2.5232320505877746|\n",
      "|1895-01-01|     Wah|             4.011|-2.5232320505877746|\n",
      "+----------+--------+------------------+-------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# Optimized Anomaly Detection\n",
    "\n",
    "def detect_temperature_anomalies(df, country_name=\"Pakistan\", z_threshold=2.5):\n",
    "    \"\"\"\n",
    "    Efficient anomaly detection using window functions and broadcasting\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ” Detecting temperature anomalies for {country_name}...\")\n",
    "    \n",
    "    # Filter for specific country and cache for multiple operations\n",
    "    country_df = df.filter(col(\"Country\") == country_name).cache()\n",
    "    \n",
    "    # Calculate statistics using Spark SQL for better optimization\n",
    "    stats_df = country_df.agg(\n",
    "        mean(\"AverageTemperature\").alias(\"mean_temp\"),\n",
    "        stddev(\"AverageTemperature\").alias(\"stddev_temp\")\n",
    "    )\n",
    "    \n",
    "    # Broadcast small statistics for efficient joins\n",
    "    stats = stats_df.collect()[0]\n",
    "    mean_temp = stats[\"mean_temp\"]\n",
    "    stddev_temp = stats[\"stddev_temp\"]\n",
    "    \n",
    "    print(f\"ğŸ“Š Temperature stats for {country_name}:\")\n",
    "    print(f\"   Mean: {mean_temp:.2f}Â°C, Std Dev: {stddev_temp:.2f}Â°C\")\n",
    "    \n",
    "    # Calculate z-scores efficiently\n",
    "    anomalies_df = country_df.withColumn(\n",
    "        \"z_score\",\n",
    "        (col(\"AverageTemperature\") - mean_temp) / stddev_temp\n",
    "    ).withColumn(\n",
    "        \"is_anomaly\",\n",
    "        (col(\"z_score\") > z_threshold) | (col(\"z_score\") < -z_threshold)\n",
    "    )\n",
    "    \n",
    "    # Filter and collect anomalies\n",
    "    anomalies = anomalies_df.filter(col(\"is_anomaly\") == True) \\\n",
    "                           .orderBy(desc(\"z_score\")) \\\n",
    "                           .limit(100)\n",
    "    \n",
    "    anomaly_count = anomalies.count()\n",
    "    print(f\"ğŸš¨ Found {anomaly_count} temperature anomalies (|z-score| > {z_threshold})\")\n",
    "    \n",
    "    return anomalies_df, anomalies, mean_temp, stddev_temp\n",
    "\n",
    "# Detect anomalies for Pakistan\n",
    "anomalies_df, top_anomalies, mean_temp, stddev_temp = detect_temperature_anomalies(climate_df)\n",
    "\n",
    "# Show top anomalies\n",
    "print(\"\\nğŸ”¥ Top Temperature Anomalies:\")\n",
    "top_anomalies.select(\"dt\", \"City\", \"AverageTemperature\", \"z_score\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒ¡ï¸ Analyzing climate patterns...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ˆ Analyzed patterns for 159 countries\n",
      "\n",
      "ğŸŒ Top 10 Warmest Countries (Average):\n",
      "+-------------+------------------+------------------+-------------+\n",
      "|      Country|  overall_avg_temp|          temp_std|total_records|\n",
      "+-------------+------------------+------------------+-------------+\n",
      "|     Djibouti|29.152790108564513|3.2324564540287923|         1797|\n",
      "|        Niger| 28.14555167114869| 3.519248026492215|         5763|\n",
      "|        Sudan|28.072830827505804| 2.935183581307311|        18798|\n",
      "| Burkina Faso|27.815294546436274|1.9054975735686468|         3954|\n",
      "|         Mali|27.590490834668028|2.7273737990041114|         5931|\n",
      "|         Chad|27.189829394812683|2.0257559835744106|         3786|\n",
      "|Guinea Bissau|27.057185462319627| 1.770712792916202|         1977|\n",
      "|   Mauritania|27.021904935064928|3.3754219372504335|         1977|\n",
      "|        Benin|26.975880208333283|1.6951569235613595|        11862|\n",
      "|     Cambodia| 26.91813629772836|1.3800966067235185|         2265|\n",
      "+-------------+------------------+------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "ğŸ‡µğŸ‡° Pakistan Monthly Temperature Patterns:\n",
      "+-------------+------------------+------------------+-------------+\n",
      "|      Country|  overall_avg_temp|          temp_std|total_records|\n",
      "+-------------+------------------+------------------+-------------+\n",
      "|     Djibouti|29.152790108564513|3.2324564540287923|         1797|\n",
      "|        Niger| 28.14555167114869| 3.519248026492215|         5763|\n",
      "|        Sudan|28.072830827505804| 2.935183581307311|        18798|\n",
      "| Burkina Faso|27.815294546436274|1.9054975735686468|         3954|\n",
      "|         Mali|27.590490834668028|2.7273737990041114|         5931|\n",
      "|         Chad|27.189829394812683|2.0257559835744106|         3786|\n",
      "|Guinea Bissau|27.057185462319627| 1.770712792916202|         1977|\n",
      "|   Mauritania|27.021904935064928|3.3754219372504335|         1977|\n",
      "|        Benin|26.975880208333283|1.6951569235613595|        11862|\n",
      "|     Cambodia| 26.91813629772836|1.3800966067235185|         2265|\n",
      "+-------------+------------------+------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "ğŸ‡µğŸ‡° Pakistan Monthly Temperature Patterns:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:===================================>                  (131 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+------------------+\n",
      "|month|          avg_temp|  temp_variability|\n",
      "+-----+------------------+------------------+\n",
      "|    1|12.109314803863583| 2.575387282728381|\n",
      "|    2|14.761605055292264| 2.984342390398989|\n",
      "|    3|20.232187194525885|3.3044091885091045|\n",
      "|    4|26.016165491937812|3.2813482040092405|\n",
      "|    5|31.235977692231717|2.9884813895178284|\n",
      "|    6| 33.50882330097082| 2.236994977424619|\n",
      "|    7| 32.28132862224791|1.9466240864924103|\n",
      "|    8|30.908044126129617|1.9012356280201455|\n",
      "|    9|29.182587376763905|  2.09110357916176|\n",
      "|   10| 24.82043684673241|2.4556742977411927|\n",
      "|   11| 18.70908342194295| 2.614305892506581|\n",
      "|   12|13.453640402265156|2.5413524863691674|\n",
      "+-----+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Optimized Climate Pattern Analysis - Fixed\n",
    "\n",
    "def analyze_climate_patterns(df):\n",
    "    \"\"\"\n",
    "    Efficient climate pattern analysis using optimized aggregations\n",
    "    \"\"\"\n",
    "    print(\"ğŸŒ¡ï¸ Analyzing climate patterns...\")\n",
    "    \n",
    "    # Monthly patterns with efficient grouping\n",
    "    monthly_patterns = df.groupBy(\"Country\", \"month\") \\\n",
    "                        .agg(\n",
    "                            avg(\"AverageTemperature\").alias(\"avg_temp\"),\n",
    "                            stddev(\"AverageTemperature\").alias(\"temp_variability\"),\n",
    "                            count(\"*\").alias(\"data_points\")\n",
    "                        ) \\\n",
    "                        .filter(col(\"data_points\") > 10) \\\n",
    "                        .cache()\n",
    "    \n",
    "    # Yearly trends with window functions for better performance\n",
    "    yearly_trends = df.groupBy(\"Country\", \"year\") \\\n",
    "                     .agg(\n",
    "                         avg(\"AverageTemperature\").alias(\"yearly_avg_temp\"),\n",
    "                         count(\"*\").alias(\"yearly_data_points\")\n",
    "                     ) \\\n",
    "                     .filter(col(\"yearly_data_points\") > 12) \\\n",
    "                     .cache()\n",
    "    \n",
    "    # Country-wise temperature statistics - using properly imported functions\n",
    "    country_stats = df.groupBy(\"Country\") \\\n",
    "                     .agg(\n",
    "                         avg(\"AverageTemperature\").alias(\"overall_avg_temp\"),\n",
    "                         spark_min(\"AverageTemperature\").alias(\"min_temp\"),\n",
    "                         spark_max(\"AverageTemperature\").alias(\"max_temp\"),\n",
    "                         stddev(\"AverageTemperature\").alias(\"temp_std\"),\n",
    "                         count(\"*\").alias(\"total_records\")\n",
    "                     ) \\\n",
    "                     .filter(col(\"total_records\") > 100) \\\n",
    "                     .orderBy(desc(\"overall_avg_temp\")) \\\n",
    "                     .cache()\n",
    "    \n",
    "    print(f\"ğŸ“ˆ Analyzed patterns for {country_stats.count()} countries\")\n",
    "    \n",
    "    return monthly_patterns, yearly_trends, country_stats\n",
    "\n",
    "# Perform pattern analysis\n",
    "monthly_patterns, yearly_trends, country_stats = analyze_climate_patterns(climate_df)\n",
    "\n",
    "# Show insights\n",
    "print(\"\\nğŸŒ Top 10 Warmest Countries (Average):\")\n",
    "country_stats.select(\"Country\", \"overall_avg_temp\", \"temp_std\", \"total_records\").show(10)\n",
    "\n",
    "print(\"\\nğŸ‡µğŸ‡° Pakistan Monthly Temperature Patterns:\")\n",
    "monthly_patterns.filter(col(\"Country\") == \"Pakistan\") \\\n",
    "                .orderBy(\"month\") \\\n",
    "                .select(\"month\", \"avg_temp\", \"temp_variability\") \\\n",
    "                .show(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš¡ Spark Performance Metrics:\n",
      "   Active Spark Context: local-1755329434131\n",
      "   Default Parallelism: 2\n",
      "   Spark UI: http://34fd580d-f292-40bc-8d12-3ae5cb26456d.internal.cloudapp.net:4040\n",
      "\n",
      "ğŸ’¾ Cached DataFrames:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - climate_df (manual cache)\n",
      "   - monthly_patterns (if exists)\n",
      "   - yearly_trends (if exists)\n",
      "   - country_stats (if exists)\n",
      "âœ… Applied runtime optimizations\n"
     ]
    }
   ],
   "source": [
    "# Performance Monitoring and Resource Management\n",
    "\n",
    "def show_spark_performance_metrics():\n",
    "    \"\"\"Display Spark performance metrics and resource usage\"\"\"\n",
    "    print(\"âš¡ Spark Performance Metrics:\")\n",
    "    print(f\"   Active Spark Context: {spark.sparkContext.applicationId}\")\n",
    "    print(f\"   Default Parallelism: {spark.sparkContext.defaultParallelism}\")\n",
    "    print(f\"   Spark UI: {spark.sparkContext.uiWebUrl}\")\n",
    "    \n",
    "    # Show cached DataFrames info\n",
    "    print(\"\\nğŸ’¾ Cached DataFrames:\")\n",
    "    try:\n",
    "        # Get storage level info for cached datasets\n",
    "        cached_count = 0\n",
    "        for table in spark.catalog.listTables():\n",
    "            if table.isTemporary:\n",
    "                cached_count += 1\n",
    "                print(f\"   - {table.name}\")\n",
    "        \n",
    "        if cached_count == 0:\n",
    "            print(\"   - climate_df (manual cache)\")\n",
    "            print(\"   - monthly_patterns (if exists)\")\n",
    "            print(\"   - yearly_trends (if exists)\")\n",
    "            print(\"   - country_stats (if exists)\")\n",
    "    except Exception as e:\n",
    "        print(f\"   - Unable to list cached tables: {str(e)}\")\n",
    "\n",
    "def optimize_spark_config():\n",
    "    \"\"\"Apply runtime optimizations\"\"\"\n",
    "    spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "    spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "    print(\"âœ… Applied runtime optimizations\")\n",
    "\n",
    "def cleanup_resources():\n",
    "    \"\"\"Clean up Spark resources\"\"\"\n",
    "    # Unpersist cached DataFrames\n",
    "    try:\n",
    "        climate_df.unpersist()\n",
    "        if 'monthly_patterns' in globals():\n",
    "            monthly_patterns.unpersist()\n",
    "        if 'yearly_trends' in globals():\n",
    "            yearly_trends.unpersist()\n",
    "        if 'country_stats' in globals():\n",
    "            country_stats.unpersist()\n",
    "        print(\"ğŸ§¹ Cleaned up cached DataFrames\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def stop_spark_session():\n",
    "    \"\"\"Properly stop Spark session\"\"\"\n",
    "    cleanup_resources()\n",
    "    spark.stop()\n",
    "    print(\"ğŸ›‘ Spark session stopped\")\n",
    "\n",
    "# Show current performance metrics\n",
    "show_spark_performance_metrics()\n",
    "\n",
    "# Apply optimizations\n",
    "optimize_spark_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš¡ Pre-computing aggregations for dashboard...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dashboard aggregations cached successfully\n",
      "ğŸš€ Dashboard initialized with PySpark backend\n",
      "ğŸ¯ Climate Analysis Dashboard ready!\n"
     ]
    }
   ],
   "source": [
    "# ğŸŒ Comprehensive Climate Analysis Dashboard\n",
    "\n",
    "import gradio as gr\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "class ClimateAnalysisDashboard:\n",
    "    \"\"\"\n",
    "    Comprehensive dashboard for climate data analysis with PySpark backend\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, spark_session, climate_dataframe):\n",
    "        self.spark = spark_session\n",
    "        self.df = climate_dataframe\n",
    "        self.countries = self._get_countries()\n",
    "        self._cache_aggregations()\n",
    "        print(\"ğŸš€ Dashboard initialized with PySpark backend\")\n",
    "    \n",
    "    def _get_countries(self):\n",
    "        \"\"\"Get list of countries efficiently\"\"\"\n",
    "        countries = [row['Country'] for row in \n",
    "                    self.df.select(\"Country\").distinct().orderBy(\"Country\").collect()]\n",
    "        return sorted(countries)\n",
    "    \n",
    "    def _cache_aggregations(self):\n",
    "        \"\"\"Pre-compute and cache common aggregations for faster dashboard response\"\"\"\n",
    "        print(\"âš¡ Pre-computing aggregations for dashboard...\")\n",
    "        \n",
    "        # Global monthly patterns\n",
    "        self.global_monthly = self.df.groupBy(\"month\") \\\n",
    "                                   .agg(avg(\"AverageTemperature\").alias(\"global_avg_temp\")) \\\n",
    "                                   .orderBy(\"month\") \\\n",
    "                                   .cache()\n",
    "        \n",
    "        # Country-wise statistics\n",
    "        self.country_stats = self.df.groupBy(\"Country\") \\\n",
    "                                  .agg(\n",
    "                                      avg(\"AverageTemperature\").alias(\"avg_temp\"),\n",
    "                                      spark_min(\"AverageTemperature\").alias(\"min_temp\"),\n",
    "                                      spark_max(\"AverageTemperature\").alias(\"max_temp\"),\n",
    "                                      stddev(\"AverageTemperature\").alias(\"std_temp\"),\n",
    "                                      count(\"*\").alias(\"record_count\")\n",
    "                                  ) \\\n",
    "                                  .filter(col(\"record_count\") > 50) \\\n",
    "                                  .cache()\n",
    "        \n",
    "        # Yearly global trends\n",
    "        self.global_yearly = self.df.groupBy(\"year\") \\\n",
    "                                  .agg(avg(\"AverageTemperature\").alias(\"global_yearly_avg\")) \\\n",
    "                                  .filter(col(\"year\") >= 1900) \\\n",
    "                                  .orderBy(\"year\") \\\n",
    "                                  .cache()\n",
    "        \n",
    "        # Force caching\n",
    "        self.global_monthly.count()\n",
    "        self.country_stats.count()\n",
    "        self.global_yearly.count()\n",
    "        print(\"âœ… Dashboard aggregations cached successfully\")\n",
    "    \n",
    "    def get_country_analysis(self, country, analysis_type, year_range):\n",
    "        \"\"\"Comprehensive country analysis\"\"\"\n",
    "        start_year, end_year = year_range\n",
    "        \n",
    "        # Filter data for country and year range\n",
    "        country_df = self.df.filter(\n",
    "            (col(\"Country\") == country) & \n",
    "            (col(\"year\") >= start_year) & \n",
    "            (col(\"year\") <= end_year)\n",
    "        )\n",
    "        \n",
    "        if analysis_type == \"Monthly Patterns\":\n",
    "            return self._get_monthly_pattern(country_df, country)\n",
    "        elif analysis_type == \"Yearly Trends\":\n",
    "            return self._get_yearly_trend(country_df, country)\n",
    "        elif analysis_type == \"Temperature Anomalies\":\n",
    "            return self._get_anomalies(country_df, country)\n",
    "        elif analysis_type == \"Seasonal Analysis\":\n",
    "            return self._get_seasonal_analysis(country_df, country)\n",
    "        else:\n",
    "            return self._get_overview(country_df, country)\n",
    "    \n",
    "    def _get_monthly_pattern(self, country_df, country):\n",
    "        \"\"\"Monthly temperature patterns\"\"\"\n",
    "        monthly_data = country_df.groupBy(\"month\") \\\n",
    "                                .agg(\n",
    "                                    avg(\"AverageTemperature\").alias(\"avg_temp\"),\n",
    "                                    stddev(\"AverageTemperature\").alias(\"std_temp\"),\n",
    "                                    count(\"*\").alias(\"data_points\")\n",
    "                                ) \\\n",
    "                                .orderBy(\"month\") \\\n",
    "                                .toPandas()\n",
    "        \n",
    "        if monthly_data.empty:\n",
    "            return go.Figure().add_annotation(text=\"No data available\", \n",
    "                                            xref=\"paper\", yref=\"paper\", x=0.5, y=0.5)\n",
    "        \n",
    "        fig = go.Figure()\n",
    "        \n",
    "        # Add temperature line with error bars\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=monthly_data['month'],\n",
    "            y=monthly_data['avg_temp'],\n",
    "            error_y=dict(type='data', array=monthly_data['std_temp']),\n",
    "            mode='lines+markers',\n",
    "            name='Average Temperature',\n",
    "            line=dict(color='royalblue', width=3),\n",
    "            marker=dict(size=8)\n",
    "        ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=f\"ğŸ“… Monthly Temperature Pattern - {country}\",\n",
    "            xaxis_title=\"Month\",\n",
    "            yaxis_title=\"Temperature (Â°C)\",\n",
    "            template=\"plotly_white\",\n",
    "            height=500\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    def _get_yearly_trend(self, country_df, country):\n",
    "        \"\"\"Long-term yearly trends\"\"\"\n",
    "        yearly_data = country_df.groupBy(\"year\") \\\n",
    "                               .agg(avg(\"AverageTemperature\").alias(\"yearly_avg\")) \\\n",
    "                               .orderBy(\"year\") \\\n",
    "                               .toPandas()\n",
    "        \n",
    "        if yearly_data.empty:\n",
    "            return go.Figure().add_annotation(text=\"No data available\", \n",
    "                                            xref=\"paper\", yref=\"paper\", x=0.5, y=0.5)\n",
    "        \n",
    "        fig = go.Figure()\n",
    "        \n",
    "        # Add yearly trend line\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=yearly_data['year'],\n",
    "            y=yearly_data['yearly_avg'],\n",
    "            mode='lines+markers',\n",
    "            name='Yearly Average',\n",
    "            line=dict(color='darkorange', width=2),\n",
    "            marker=dict(size=4)\n",
    "        ))\n",
    "        \n",
    "        # Add trend line\n",
    "        if len(yearly_data) > 5:\n",
    "            z = np.polyfit(yearly_data['year'], yearly_data['yearly_avg'], 1)\n",
    "            p = np.poly1d(z)\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=yearly_data['year'],\n",
    "                y=p(yearly_data['year']),\n",
    "                mode='lines',\n",
    "                name='Trend Line',\n",
    "                line=dict(color='red', width=2, dash='dash')\n",
    "            ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=f\"ğŸ“ˆ Long-term Temperature Trend - {country}\",\n",
    "            xaxis_title=\"Year\",\n",
    "            yaxis_title=\"Temperature (Â°C)\",\n",
    "            template=\"plotly_white\",\n",
    "            height=500\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    def _get_anomalies(self, country_df, country):\n",
    "        \"\"\"Temperature anomalies detection\"\"\"\n",
    "        # Calculate statistics\n",
    "        stats = country_df.agg(\n",
    "            mean(\"AverageTemperature\").alias(\"mean_temp\"),\n",
    "            stddev(\"AverageTemperature\").alias(\"std_temp\")\n",
    "        ).collect()[0]\n",
    "        \n",
    "        mean_temp = stats[\"mean_temp\"]\n",
    "        std_temp = stats[\"std_temp\"]\n",
    "        \n",
    "        # Get anomalies\n",
    "        anomalies_data = country_df.withColumn(\n",
    "            \"z_score\", (col(\"AverageTemperature\") - mean_temp) / std_temp\n",
    "        ).withColumn(\n",
    "            \"is_anomaly\", (col(\"z_score\") > 2.5) | (col(\"z_score\") < -2.5)\n",
    "        ).select(\"dt\", \"AverageTemperature\", \"z_score\", \"is_anomaly\", \"year\", \"month\") \\\n",
    "         .orderBy(\"dt\") \\\n",
    "         .limit(2000) \\\n",
    "         .toPandas()\n",
    "        \n",
    "        if anomalies_data.empty:\n",
    "            return go.Figure().add_annotation(text=\"No data available\", \n",
    "                                            xref=\"paper\", yref=\"paper\", x=0.5, y=0.5)\n",
    "        \n",
    "        anomalies_data['dt'] = pd.to_datetime(anomalies_data['dt'])\n",
    "        normal_data = anomalies_data[~anomalies_data['is_anomaly']]\n",
    "        anomaly_data = anomalies_data[anomalies_data['is_anomaly']]\n",
    "        \n",
    "        fig = go.Figure()\n",
    "        \n",
    "        # Normal data points\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=normal_data['dt'],\n",
    "            y=normal_data['AverageTemperature'],\n",
    "            mode='markers',\n",
    "            name='Normal Temperature',\n",
    "            marker=dict(color='lightblue', size=4, opacity=0.6)\n",
    "        ))\n",
    "        \n",
    "        # Anomaly points\n",
    "        if not anomaly_data.empty:\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=anomaly_data['dt'],\n",
    "                y=anomaly_data['AverageTemperature'],\n",
    "                mode='markers',\n",
    "                name='Temperature Anomalies',\n",
    "                marker=dict(\n",
    "                    color=anomaly_data['z_score'],\n",
    "                    colorscale='RdYlBu_r',\n",
    "                    size=8,\n",
    "                    colorbar=dict(title=\"Z-Score\")\n",
    "                )\n",
    "            ))\n",
    "        \n",
    "        # Add mean line\n",
    "        fig.add_hline(y=mean_temp, line_dash=\"dash\", line_color=\"green\", \n",
    "                     annotation_text=\"Mean Temperature\")\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=f\"ğŸš¨ Temperature Anomalies - {country}\",\n",
    "            xaxis_title=\"Date\",\n",
    "            yaxis_title=\"Temperature (Â°C)\",\n",
    "            template=\"plotly_white\",\n",
    "            height=500\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    def _get_seasonal_analysis(self, country_df, country):\n",
    "        \"\"\"Seasonal temperature analysis\"\"\"\n",
    "        seasonal_data = country_df.withColumn(\n",
    "            \"season\",\n",
    "            when((col(\"month\").isin([12, 1, 2])), \"Winter\")\n",
    "            .when((col(\"month\").isin([3, 4, 5])), \"Spring\")\n",
    "            .when((col(\"month\").isin([6, 7, 8])), \"Summer\")\n",
    "            .otherwise(\"Autumn\")\n",
    "        ).groupBy(\"season\", \"year\") \\\n",
    "         .agg(avg(\"AverageTemperature\").alias(\"seasonal_avg\")) \\\n",
    "         .orderBy(\"year\") \\\n",
    "         .toPandas()\n",
    "        \n",
    "        if seasonal_data.empty:\n",
    "            return go.Figure().add_annotation(text=\"No data available\", \n",
    "                                            xref=\"paper\", yref=\"paper\", x=0.5, y=0.5)\n",
    "        \n",
    "        fig = go.Figure()\n",
    "        \n",
    "        seasons = [\"Winter\", \"Spring\", \"Summer\", \"Autumn\"]\n",
    "        colors = [\"lightblue\", \"lightgreen\", \"orange\", \"brown\"]\n",
    "        \n",
    "        for season, color in zip(seasons, colors):\n",
    "            season_data = seasonal_data[seasonal_data['season'] == season]\n",
    "            if not season_data.empty:\n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=season_data['year'],\n",
    "                    y=season_data['seasonal_avg'],\n",
    "                    mode='lines+markers',\n",
    "                    name=season,\n",
    "                    line=dict(color=color, width=2),\n",
    "                    marker=dict(size=4)\n",
    "                ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=f\"ğŸ‚ Seasonal Temperature Analysis - {country}\",\n",
    "            xaxis_title=\"Year\",\n",
    "            yaxis_title=\"Temperature (Â°C)\",\n",
    "            template=\"plotly_white\",\n",
    "            height=500\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    def _get_overview(self, country_df, country):\n",
    "        \"\"\"Country overview with key statistics\"\"\"\n",
    "        stats = country_df.agg(\n",
    "            avg(\"AverageTemperature\").alias(\"avg_temp\"),\n",
    "            spark_min(\"AverageTemperature\").alias(\"min_temp\"),\n",
    "            spark_max(\"AverageTemperature\").alias(\"max_temp\"),\n",
    "            stddev(\"AverageTemperature\").alias(\"std_temp\"),\n",
    "            count(\"*\").alias(\"total_records\")\n",
    "        ).collect()[0]\n",
    "        \n",
    "        # Create a summary chart\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=(\"Temperature Distribution\", \"Monthly Averages\", \n",
    "                          \"Data Coverage\", \"Temperature Range\"),\n",
    "            specs=[[{\"type\": \"histogram\"}, {\"type\": \"bar\"}],\n",
    "                   [{\"type\": \"scatter\"}, {\"type\": \"indicator\"}]]\n",
    "        )\n",
    "        \n",
    "        # Get sample data for visualizations\n",
    "        sample_data = country_df.select(\"AverageTemperature\", \"month\", \"year\") \\\n",
    "                               .sample(0.1) \\\n",
    "                               .limit(1000) \\\n",
    "                               .toPandas()\n",
    "        \n",
    "        if not sample_data.empty:\n",
    "            # Temperature distribution\n",
    "            fig.add_trace(go.Histogram(x=sample_data['AverageTemperature'], \n",
    "                                     name=\"Temperature Distribution\"),\n",
    "                         row=1, col=1)\n",
    "            \n",
    "            # Monthly averages\n",
    "            monthly_avg = sample_data.groupby('month')['AverageTemperature'].mean()\n",
    "            fig.add_trace(go.Bar(x=monthly_avg.index, y=monthly_avg.values,\n",
    "                               name=\"Monthly Avg\"),\n",
    "                         row=1, col=2)\n",
    "            \n",
    "            # Data coverage over years\n",
    "            yearly_count = sample_data.groupby('year').size()\n",
    "            fig.add_trace(go.Scatter(x=yearly_count.index, y=yearly_count.values,\n",
    "                                   mode='lines', name=\"Data Points\"),\n",
    "                         row=2, col=1)\n",
    "        \n",
    "        # Temperature range indicator\n",
    "        fig.add_trace(go.Indicator(\n",
    "            mode=\"gauge+number+delta\",\n",
    "            value=stats[\"avg_temp\"],\n",
    "            domain={'x': [0, 1], 'y': [0, 1]},\n",
    "            title={'text': \"Avg Temperature (Â°C)\"},\n",
    "            gauge={'axis': {'range': [None, 50]},\n",
    "                   'bar': {'color': \"darkblue\"},\n",
    "                   'steps': [{'range': [0, 20], 'color': \"lightgray\"},\n",
    "                            {'range': [20, 35], 'color': \"gray\"}],\n",
    "                   'threshold': {'line': {'color': \"red\", 'width': 4},\n",
    "                               'thickness': 0.75, 'value': stats[\"avg_temp\"]}}),\n",
    "            row=2, col=2)\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=f\"ğŸ“Š Climate Overview - {country}\",\n",
    "            height=600,\n",
    "            showlegend=False\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    def get_global_comparison(self, countries_list, metric):\n",
    "        \"\"\"Global comparison between countries\"\"\"\n",
    "        if not countries_list:\n",
    "            return go.Figure().add_annotation(text=\"Please select countries to compare\", \n",
    "                                            xref=\"paper\", yref=\"paper\", x=0.5, y=0.5)\n",
    "        \n",
    "        comparison_data = self.country_stats.filter(col(\"Country\").isin(countries_list)) \\\n",
    "                                          .toPandas()\n",
    "        \n",
    "        if comparison_data.empty:\n",
    "            return go.Figure().add_annotation(text=\"No data available for selected countries\", \n",
    "                                            xref=\"paper\", yref=\"paper\", x=0.5, y=0.5)\n",
    "        \n",
    "        fig = go.Figure()\n",
    "        \n",
    "        if metric == \"Average Temperature\":\n",
    "            fig.add_trace(go.Bar(\n",
    "                x=comparison_data['Country'],\n",
    "                y=comparison_data['avg_temp'],\n",
    "                name='Average Temperature',\n",
    "                marker_color='skyblue'\n",
    "            ))\n",
    "            fig.update_layout(yaxis_title=\"Temperature (Â°C)\")\n",
    "            \n",
    "        elif metric == \"Temperature Range\":\n",
    "            fig.add_trace(go.Bar(\n",
    "                x=comparison_data['Country'],\n",
    "                y=comparison_data['max_temp'] - comparison_data['min_temp'],\n",
    "                name='Temperature Range',\n",
    "                marker_color='coral'\n",
    "            ))\n",
    "            fig.update_layout(yaxis_title=\"Temperature Range (Â°C)\")\n",
    "            \n",
    "        elif metric == \"Temperature Variability\":\n",
    "            fig.add_trace(go.Bar(\n",
    "                x=comparison_data['Country'],\n",
    "                y=comparison_data['std_temp'],\n",
    "                name='Temperature Std Dev',\n",
    "                marker_color='lightgreen'\n",
    "            ))\n",
    "            fig.update_layout(yaxis_title=\"Standard Deviation (Â°C)\")\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=f\"ğŸŒ Global Comparison - {metric}\",\n",
    "            xaxis_title=\"Countries\",\n",
    "            template=\"plotly_white\",\n",
    "            height=500\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    def get_performance_metrics(self):\n",
    "        \"\"\"Get Spark performance metrics\"\"\"\n",
    "        try:\n",
    "            app_id = self.spark.sparkContext.applicationId\n",
    "            parallelism = self.spark.sparkContext.defaultParallelism\n",
    "            ui_url = self.spark.sparkContext.uiWebUrl\n",
    "            \n",
    "            # Count cached DataFrames (approximate)\n",
    "            cached_rdds = 4  # We know we cache: climate_df, global_monthly, country_stats, global_yearly\n",
    "            \n",
    "            metrics = f\"\"\"\n",
    "            ğŸš€ **Spark Performance Dashboard**\n",
    "            \n",
    "            - **Application ID**: {app_id}\n",
    "            - **Default Parallelism**: {parallelism}\n",
    "            - **Cached DataFrames**: {cached_rdds}\n",
    "            - **Spark UI**: [Open Dashboard]({ui_url})\n",
    "            \n",
    "            âœ… **Status**: All systems operational\n",
    "            \"\"\"\n",
    "            \n",
    "            return metrics\n",
    "        except Exception as e:\n",
    "            return f\"âŒ **Error**: {str(e)}\"\n",
    "\n",
    "# Initialize the dashboard\n",
    "dashboard = ClimateAnalysisDashboard(spark, climate_df)\n",
    "print(\"ğŸ¯ Climate Analysis Dashboard ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ Creating enhanced climate dashboard...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Enhanced Dashboard created successfully!\n",
      "ğŸ“Š Features:\n",
      "   - Real-time country analysis\n",
      "   - Global comparisons (159 countries)\n",
      "   - ğŸ”® AI temperature predictions (demo mode)\n",
      "   - System monitoring\n",
      "   - Interactive visualizations\n",
      "\n",
      "ğŸš€ Ready to launch! Use enhanced_dashboard.launch() when needed.\n"
     ]
    }
   ],
   "source": [
    "# ğŸš€ ENHANCED DASHBOARD WITH INTEGRATED TEMPERATURE PREDICTIONS\n",
    "\n",
    "import gradio as gr\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Mock ML Evaluator for temperature predictions\n",
    "class MockMLEvaluator:\n",
    "    \"\"\"Mock ML evaluator for temperature predictions until real ML models are implemented\"\"\"\n",
    "    \n",
    "    def predict_temperature(self, country, forecast_years, model_type):\n",
    "        \"\"\"Generate mock predictions with realistic patterns\"\"\"\n",
    "        try:\n",
    "            # Get historical data for the country\n",
    "            historical_data = dashboard.get_country_historical_data(country)\n",
    "            if not historical_data:\n",
    "                raise ValueError(f\"No historical data available for {country}\")\n",
    "            \n",
    "            # Create mock predictions based on historical trends\n",
    "            last_year = 2013  # Based on the dataset\n",
    "            future_years = list(range(last_year + 1, last_year + forecast_years + 1))\n",
    "            \n",
    "            # Simple trend-based prediction (mock)\n",
    "            base_temp = historical_data['avg_temp']\n",
    "            trend = 0.02 if model_type == \"Linear Regression\" else 0.03  # Mock warming trend\n",
    "            \n",
    "            predictions = []\n",
    "            for i, year in enumerate(future_years):\n",
    "                predicted_temp = base_temp + (trend * i) + np.random.normal(0, 0.5)\n",
    "                predictions.append({\n",
    "                    'year': year,\n",
    "                    'predicted_temp': predicted_temp,\n",
    "                    'confidence_upper': predicted_temp + 1.5,\n",
    "                    'confidence_lower': predicted_temp - 1.5\n",
    "                })\n",
    "            \n",
    "            pred_df = pd.DataFrame(predictions)\n",
    "            \n",
    "            # Mock metrics\n",
    "            metrics = {\n",
    "                'r2_score': 0.835 if model_type == \"Random Forest\" else 0.789,\n",
    "                'rmse': 2.1 if model_type == \"Gradient Boosting\" else 2.8,\n",
    "                'f1_score': 0.821 if model_type == \"Gradient Boosting\" else 0.756\n",
    "            }\n",
    "            \n",
    "            return pred_df, metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Prediction failed: {str(e)}\")\n",
    "\n",
    "# Initialize mock evaluator\n",
    "evaluator = MockMLEvaluator()\n",
    "\n",
    "def create_enhanced_dashboard():\n",
    "    \"\"\"Create enhanced Gradio dashboard with temperature prediction capabilities\"\"\"\n",
    "    \n",
    "    def analyze_country(country, analysis_type, start_year, end_year):\n",
    "        \"\"\"Country analysis function with error handling\"\"\"\n",
    "        try:\n",
    "            if analysis_type == \"Overview\":\n",
    "                # Use the existing _get_overview method instead of missing get_country_overview\n",
    "                fig = dashboard._get_overview(\n",
    "                    dashboard.df.filter(\n",
    "                        (col(\"Country\") == country) & \n",
    "                        (col(\"year\") >= start_year) & \n",
    "                        (col(\"year\") <= end_year)\n",
    "                    ), country\n",
    "                )\n",
    "                return fig, f\"âœ… **Analysis complete** for {country}\"\n",
    "            else:\n",
    "                fig = dashboard.get_country_analysis(country, analysis_type, (start_year, end_year))\n",
    "                return fig, f\"âœ… **{analysis_type}** analysis complete for {country}\"\n",
    "        except Exception as e:\n",
    "            # Create an error figure\n",
    "            error_fig = go.Figure()\n",
    "            error_fig.add_annotation(\n",
    "                text=f\"Error: {str(e)}\", \n",
    "                xref=\"paper\", yref=\"paper\", \n",
    "                x=0.5, y=0.5,\n",
    "                showarrow=False,\n",
    "                font=dict(size=16, color=\"red\")\n",
    "            )\n",
    "            return error_fig, f\"âŒ **Error**: {str(e)}\"\n",
    "    \n",
    "    def predict_temperature(country, forecast_years, model_type):\n",
    "        \"\"\"Temperature prediction function with enhanced error handling\"\"\"\n",
    "        try:\n",
    "            # Use the mock evaluator for prediction\n",
    "            predictions, metrics = evaluator.predict_temperature(country, forecast_years, model_type)\n",
    "            \n",
    "            # Create prediction visualization\n",
    "            fig = go.Figure()\n",
    "            \n",
    "            # Historical data\n",
    "            historical_data = dashboard.get_country_historical_data(country)\n",
    "            if historical_data:\n",
    "                # Get recent historical trend for visualization\n",
    "                recent_data = dashboard.df.filter(\n",
    "                    (col(\"Country\") == country) & \n",
    "                    (col(\"year\") >= 2000)\n",
    "                ).groupBy(\"year\") \\\n",
    "                 .agg(avg(\"AverageTemperature\").alias(\"yearly_avg\")) \\\n",
    "                 .orderBy(\"year\") \\\n",
    "                 .toPandas()\n",
    "                \n",
    "                if not recent_data.empty:\n",
    "                    fig.add_trace(go.Scatter(\n",
    "                        x=recent_data['year'],\n",
    "                        y=recent_data['yearly_avg'],\n",
    "                        mode='lines',\n",
    "                        name='Historical Data',\n",
    "                        line=dict(color='blue', width=2)\n",
    "                    ))\n",
    "            \n",
    "            # Prediction line\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=predictions['year'],\n",
    "                y=predictions['predicted_temp'],\n",
    "                mode='lines+markers',\n",
    "                name=f'{model_type} Prediction',\n",
    "                line=dict(color='red', width=3),\n",
    "                marker=dict(size=6)\n",
    "            ))\n",
    "            \n",
    "            # Confidence intervals\n",
    "            if 'confidence_upper' in predictions.columns and 'confidence_lower' in predictions.columns:\n",
    "                fig.add_trace(go.Scatter(\n",
    "                    x=predictions['year'].tolist() + predictions['year'][::-1].tolist(),\n",
    "                    y=predictions['confidence_upper'].tolist() + predictions['confidence_lower'][::-1].tolist(),\n",
    "                    fill='toself',\n",
    "                    fillcolor='rgba(255, 0, 0, 0.2)',\n",
    "                    line=dict(color='rgba(255, 255, 255, 0)'),\n",
    "                    name='Confidence Interval',\n",
    "                    showlegend=True\n",
    "                ))\n",
    "            \n",
    "            fig.update_layout(\n",
    "                title=f\"ğŸ”® Temperature Prediction for {country} ({forecast_years} years)\",\n",
    "                xaxis_title=\"Year\",\n",
    "                yaxis_title=\"Temperature (Â°C)\",\n",
    "                template=\"plotly_white\",\n",
    "                height=500\n",
    "            )\n",
    "            \n",
    "            # Model performance info\n",
    "            performance_info = f\"\"\"\n",
    "            **ğŸ¤– Model Performance - {model_type}**\n",
    "            - **RÂ² Score**: {metrics.get('r2_score', 0):.3f}\n",
    "            - **RMSE**: Â±{metrics.get('rmse', 0):.2f}Â°C\n",
    "            - **F1 Score**: {metrics.get('f1_score', 0):.3f}\n",
    "            - **Forecast Period**: {forecast_years} years\n",
    "            - **Status**: Mock predictions (demo mode)\n",
    "            \"\"\"\n",
    "            \n",
    "            return fig, performance_info\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_fig = go.Figure()\n",
    "            error_fig.add_annotation(\n",
    "                text=f\"Prediction Error: {str(e)}\", \n",
    "                xref=\"paper\", yref=\"paper\", \n",
    "                x=0.5, y=0.5,\n",
    "                showarrow=False,\n",
    "                font=dict(size=16, color=\"red\")\n",
    "            )\n",
    "            return error_fig, f\"âŒ **Error**: {str(e)}\"\n",
    "    \n",
    "    def compare_countries(countries, metric):\n",
    "        \"\"\"Country comparison function\"\"\"\n",
    "        try:\n",
    "            fig = dashboard.get_global_comparison(countries, metric)\n",
    "            return fig\n",
    "        except Exception as e:\n",
    "            error_fig = go.Figure()\n",
    "            error_fig.add_annotation(\n",
    "                text=f\"Error: {str(e)}\", \n",
    "                xref=\"paper\", yref=\"paper\", \n",
    "                x=0.5, y=0.5,\n",
    "                showarrow=False,\n",
    "                font=dict(size=16, color=\"red\")\n",
    "            )\n",
    "            return error_fig\n",
    "    \n",
    "    # Create the Gradio interface\n",
    "    with gr.Blocks(\n",
    "        title=\"ğŸŒ Enhanced Climate Analysis Dashboard\",\n",
    "        theme=gr.themes.Soft(),\n",
    "        css=\"\"\"\n",
    "        .gradio-container {\n",
    "            font-family: 'Segoe UI', system-ui, sans-serif;\n",
    "        }\n",
    "        .tab-nav button {\n",
    "            font-size: 14px;\n",
    "            font-weight: 500;\n",
    "        }\n",
    "        \"\"\"\n",
    "    ) as interface:\n",
    "        \n",
    "        # Header\n",
    "        gr.Markdown(\"\"\"\n",
    "        # ğŸŒ Enhanced Climate Analysis Dashboard\n",
    "        \n",
    "        **ğŸ”¥ NEW**: AI-powered temperature predictions | **âš¡ Engine**: PySpark 4.0.0 | **ğŸ“Š Dataset**: Global Temperature Analysis\n",
    "        \n",
    "        Explore climate patterns, compare countries, and predict future temperatures using advanced machine learning models.\n",
    "        \"\"\")\n",
    "        \n",
    "        with gr.Tabs():\n",
    "            # Tab 1: Country Analysis\n",
    "            with gr.Tab(\"ğŸƒâ€â™‚ï¸ Country Analysis\"):\n",
    "                gr.Markdown(\"### Detailed climate analysis for individual countries\")\n",
    "                \n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=1):\n",
    "                        country_input = gr.Dropdown(\n",
    "                            choices=dashboard.countries,\n",
    "                            value=\"Pakistan\",\n",
    "                            label=\"ğŸŒ Select Country\",\n",
    "                            filterable=True\n",
    "                        )\n",
    "                        \n",
    "                        analysis_type = gr.Radio(\n",
    "                            choices=[\n",
    "                                \"Overview\",\n",
    "                                \"Monthly Patterns\", \n",
    "                                \"Yearly Trends\", \n",
    "                                \"Temperature Anomalies\",\n",
    "                                \"Seasonal Analysis\"\n",
    "                            ],\n",
    "                            value=\"Overview\",\n",
    "                            label=\"ğŸ“Š Analysis Type\"\n",
    "                        )\n",
    "                        \n",
    "                        start_year = gr.Slider(\n",
    "                            minimum=1750, maximum=2013, value=1900,\n",
    "                            step=1, label=\"ğŸ“… Start Year\"\n",
    "                        )\n",
    "                        \n",
    "                        end_year = gr.Slider(\n",
    "                            minimum=1750, maximum=2013, value=2013,\n",
    "                            step=1, label=\"ğŸ“… End Year\"  \n",
    "                        )\n",
    "                        \n",
    "                        analyze_btn = gr.Button(\"ğŸ” Analyze\", variant=\"primary\")\n",
    "                    \n",
    "                    with gr.Column(scale=2):\n",
    "                        country_plot = gr.Plot(label=\"ğŸ“Š Analysis Results\")\n",
    "                        analysis_info = gr.Markdown(\"\")\n",
    "                \n",
    "                # Connect analysis function\n",
    "                analyze_btn.click(\n",
    "                    fn=analyze_country,\n",
    "                    inputs=[country_input, analysis_type, start_year, end_year],\n",
    "                    outputs=[country_plot, analysis_info]\n",
    "                )\n",
    "            \n",
    "            # Tab 2: Temperature Predictions (FIXED!)\n",
    "            with gr.Tab(\"ğŸ”® AI Temperature Predictions\"):\n",
    "                gr.Markdown(\"\"\"\n",
    "                ### AI-Powered Climate Predictions\n",
    "                \n",
    "                **ğŸ¤– Machine Learning Models**: Linear Regression, Random Forest, Gradient Boosting  \n",
    "                **ğŸ“Š Status**: Demo mode with mock predictions  \n",
    "                **âš¡ Technology**: Optimized PySpark processing\n",
    "                \"\"\")\n",
    "                \n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=1):\n",
    "                        pred_country = gr.Dropdown(\n",
    "                            choices=dashboard.countries,\n",
    "                            value=\"Pakistan\",\n",
    "                            label=\"ğŸŒ Select Country\",\n",
    "                            filterable=True\n",
    "                        )\n",
    "                        \n",
    "                        forecast_years = gr.Slider(\n",
    "                            minimum=1, maximum=20, value=10,\n",
    "                            step=1, label=\"ğŸ“… Forecast Years\"\n",
    "                        )\n",
    "                        \n",
    "                        model_type = gr.Radio(\n",
    "                            choices=[\"Linear Regression\", \"Random Forest\", \"Gradient Boosting\"],\n",
    "                            value=\"Linear Regression\",\n",
    "                            label=\"ğŸ¤– ML Model\"\n",
    "                        )\n",
    "                        \n",
    "                        predict_btn = gr.Button(\"ğŸ”® Generate Prediction\", variant=\"primary\")\n",
    "                        \n",
    "                        gr.Markdown(\"\"\"\n",
    "                        **ğŸ’¡ Model Information:**\n",
    "                        - **Linear Regression**: Fast, interpretable, good for trends\n",
    "                        - **Random Forest**: Robust, handles complexity well\n",
    "                        - **Gradient Boosting**: High accuracy, advanced ensemble method\n",
    "                        \n",
    "                        *Note: Currently running in demo mode with mock predictions*\n",
    "                        \"\"\")\n",
    "                    \n",
    "                    with gr.Column(scale=2):\n",
    "                        prediction_plot = gr.Plot(label=\"ğŸ”® Temperature Prediction\")\n",
    "                        prediction_info = gr.Markdown(\"\")\n",
    "                \n",
    "                # Connect prediction function\n",
    "                predict_btn.click(\n",
    "                    fn=predict_temperature,\n",
    "                    inputs=[pred_country, forecast_years, model_type],\n",
    "                    outputs=[prediction_plot, prediction_info]\n",
    "                )\n",
    "            \n",
    "            # Tab 3: Global Comparison\n",
    "            with gr.Tab(\"ğŸŒ Global Comparison\"):\n",
    "                gr.Markdown(\"### Compare climate metrics across multiple countries\")\n",
    "                \n",
    "                with gr.Row():\n",
    "                    with gr.Column(scale=1):\n",
    "                        countries_input = gr.CheckboxGroup(\n",
    "                            choices=dashboard.countries,\n",
    "                            value=[\"Pakistan\", \"India\", \"United States\", \"China\"],\n",
    "                            label=\"ğŸŒ Select Countries\"\n",
    "                        )\n",
    "                        \n",
    "                        metric_input = gr.Radio(\n",
    "                            choices=[\"Average Temperature\", \"Temperature Range\", \"Temperature Variability\"],\n",
    "                            value=\"Average Temperature\",\n",
    "                            label=\"ğŸ“Š Comparison Metric\"\n",
    "                        )\n",
    "                        \n",
    "                        compare_btn = gr.Button(\"ğŸ”„ Compare Countries\", variant=\"primary\")\n",
    "                    \n",
    "                    with gr.Column(scale=2):\n",
    "                        comparison_plot = gr.Plot(label=\"ğŸŒ Global Comparison\")\n",
    "                \n",
    "                # Connect comparison function\n",
    "                compare_btn.click(\n",
    "                    fn=compare_countries,\n",
    "                    inputs=[countries_input, metric_input],\n",
    "                    outputs=comparison_plot\n",
    "                )\n",
    "            \n",
    "            # Tab 4: System Status\n",
    "            with gr.Tab(\"âš¡ System Status\"):\n",
    "                gr.Markdown(\"### PySpark Performance & System Monitoring\")\n",
    "                \n",
    "                performance_display = gr.Markdown(\n",
    "                    value=dashboard.get_performance_metrics(),\n",
    "                    label=\"ğŸ”§ System Metrics\"\n",
    "                )\n",
    "                \n",
    "                refresh_btn = gr.Button(\"ğŸ”„ Refresh Metrics\", variant=\"secondary\")\n",
    "                refresh_btn.click(\n",
    "                    fn=lambda: dashboard.get_performance_metrics(),\n",
    "                    outputs=performance_display\n",
    "                )\n",
    "                \n",
    "                gr.Markdown(f\"\"\"\n",
    "                ### ğŸ“Š Enhanced Dataset Information\n",
    "                - **Source**: Global Land Temperatures by City\n",
    "                - **Processing Engine**: Apache Spark 4.0.0\n",
    "                - **Total Countries**: {len(dashboard.countries)}\n",
    "                - **ML Capabilities**: Temperature prediction (demo mode)\n",
    "                - **Cache Strategy**: Intelligent DataFrame caching\n",
    "                - **Optimization**: Adaptive Query Execution enabled\n",
    "                \n",
    "                ### ğŸ¯ Available Operations\n",
    "                - âœ… Real-time country analysis\n",
    "                - âœ… Multi-country comparisons ({len(dashboard.countries)} countries)\n",
    "                - âœ… ğŸ”® AI temperature predictions (demo)\n",
    "                - âœ… Anomaly detection\n",
    "                - âœ… Seasonal pattern analysis\n",
    "                - âœ… Interactive visualizations\n",
    "                \"\"\")\n",
    "        \n",
    "        # Footer\n",
    "        gr.Markdown(f\"\"\"\n",
    "        ---\n",
    "        **ğŸš€ Powered by**: PySpark + Gradio | **ğŸ“Š Dataset**: {len(dashboard.countries)} Countries | **âš¡ Status**: Ready for Analysis\n",
    "        \"\"\")\n",
    "    \n",
    "    return interface\n",
    "\n",
    "# Add missing method to dashboard class\n",
    "def get_country_historical_data(self, country):\n",
    "    \"\"\"Get historical data summary for a country\"\"\"\n",
    "    try:\n",
    "        historical_stats = self.df.filter(col(\"Country\") == country) \\\n",
    "                                 .agg(avg(\"AverageTemperature\").alias(\"avg_temp\")) \\\n",
    "                                 .collect()[0]\n",
    "        \n",
    "        return {\n",
    "            'avg_temp': historical_stats['avg_temp'],\n",
    "            'country': country\n",
    "        }\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Add the method to the existing dashboard instance\n",
    "dashboard.get_country_historical_data = get_country_historical_data.__get__(dashboard, dashboard.__class__)\n",
    "\n",
    "# Create the enhanced dashboard\n",
    "print(\"ğŸ¯ Creating enhanced climate dashboard...\")\n",
    "enhanced_dashboard = create_enhanced_dashboard()\n",
    "\n",
    "print(\"âœ… Enhanced Dashboard created successfully!\")\n",
    "print(\"ğŸ“Š Features:\")\n",
    "print(\"   - Real-time country analysis\")\n",
    "print(f\"   - Global comparisons ({len(dashboard.countries)} countries)\")\n",
    "print(\"   - ğŸ”® AI temperature predictions (demo mode)\")\n",
    "print(\"   - System monitoring\")\n",
    "print(\"   - Interactive visualizations\")\n",
    "print()\n",
    "print(\"ğŸš€ Ready to launch! Use enhanced_dashboard.launch() when needed.\")\n",
    "\n",
    "# Store dashboard for manual launch\n",
    "globals()['enhanced_dashboard'] = enhanced_dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ›ï¸ DASHBOARD CONTROL PANEL\n",
      "==================================================\n",
      "Available commands:\n",
      "  ğŸ“Š quick_stats()         - Show dataset statistics\n",
      "  âš¡ check_spark_health()  - Check Spark cluster status\n",
      "  ğŸ”„ restart_dashboard()   - Restart the dashboard\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š DATASET QUICK STATS\n",
      "========================================\n",
      "ğŸ“ˆ Total Records: 8,599,212\n",
      "ğŸŒ Countries: 159\n",
      "ğŸ“… Date Range: 1743-11-01 to 2013-09-01\n",
      "ğŸŒ¡ï¸ Global Average Temperature: 16.73Â°C\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_records': 8599212,\n",
       " 'countries': 159,\n",
       " 'date_range': '1743-11-01 to 2013-09-01',\n",
       " 'avg_temp': 16.727432636249816}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ğŸ›ï¸ Dashboard Management & Quick Actions\n",
    "\n",
    "def restart_dashboard():\n",
    "    \"\"\"Restart the dashboard with fresh data\"\"\"\n",
    "    try:\n",
    "        global dashboard, main_dashboard\n",
    "        print(\"ğŸ”„ Restarting dashboard...\")\n",
    "        \n",
    "        # Refresh data and recreate dashboard\n",
    "        dashboard = ClimateAnalysisDashboard(spark, climate_df)\n",
    "        main_dashboard = create_main_dashboard()\n",
    "        \n",
    "        print(\"âœ… Dashboard restarted successfully!\")\n",
    "        return main_dashboard\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error restarting dashboard: {e}\")\n",
    "        return None\n",
    "\n",
    "def quick_stats():\n",
    "    \"\"\"Show quick statistics about the dataset\"\"\"\n",
    "    try:\n",
    "        total_records = climate_df.count()\n",
    "        countries_count = climate_df.select(\"Country\").distinct().count()\n",
    "        date_range = climate_df.agg(\n",
    "            spark_min(\"dt\").alias(\"min_date\"),\n",
    "            spark_max(\"dt\").alias(\"max_date\")\n",
    "        ).collect()[0]\n",
    "        \n",
    "        avg_temp = climate_df.agg(avg(\"AverageTemperature\")).collect()[0][0]\n",
    "        \n",
    "        print(\"ğŸ“Š DATASET QUICK STATS\")\n",
    "        print(\"=\" * 40)\n",
    "        print(f\"ğŸ“ˆ Total Records: {total_records:,}\")\n",
    "        print(f\"ğŸŒ Countries: {countries_count}\")\n",
    "        print(f\"ğŸ“… Date Range: {date_range['min_date']} to {date_range['max_date']}\")\n",
    "        print(f\"ğŸŒ¡ï¸ Global Average Temperature: {avg_temp:.2f}Â°C\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        return {\n",
    "            \"total_records\": total_records,\n",
    "            \"countries\": countries_count,\n",
    "            \"date_range\": f\"{date_range['min_date']} to {date_range['max_date']}\",\n",
    "            \"avg_temp\": avg_temp\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error getting stats: {e}\")\n",
    "        return {}\n",
    "\n",
    "def check_spark_health():\n",
    "    \"\"\"Check Spark cluster health\"\"\"\n",
    "    try:\n",
    "        print(\"âš¡ SPARK HEALTH CHECK\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # Test basic operations\n",
    "        test_count = climate_df.limit(10).count()\n",
    "        print(f\"âœ… Basic Operations: Working (test count: {test_count})\")\n",
    "        \n",
    "        # Check cache status\n",
    "        cached_tables = spark.catalog.listTables()\n",
    "        print(f\"ğŸ’¾ Cached Tables: {len(cached_tables)}\")\n",
    "        \n",
    "        # Check Spark UI\n",
    "        ui_url = spark.sparkContext.uiWebUrl\n",
    "        print(f\"ğŸŒ Spark UI: {ui_url}\")\n",
    "        \n",
    "        # Check memory usage\n",
    "        sc = spark.sparkContext\n",
    "        print(f\"ğŸ”§ Default Parallelism: {sc.defaultParallelism}\")\n",
    "        print(f\"ğŸ“± Application ID: {sc.applicationId}\")\n",
    "        \n",
    "        print(\"âœ… All systems operational!\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Spark health check failed: {e}\")\n",
    "\n",
    "# Quick actions buttons\n",
    "print(\"ğŸ›ï¸ DASHBOARD CONTROL PANEL\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Available commands:\")\n",
    "print(\"  ğŸ“Š quick_stats()         - Show dataset statistics\")\n",
    "print(\"  âš¡ check_spark_health()  - Check Spark cluster status\") \n",
    "print(\"  ğŸ”„ restart_dashboard()   - Restart the dashboard\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Show initial stats\n",
    "quick_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Ready to launch the Enhanced Climate Analysis Dashboard!\n",
      "ğŸ“‹ Make sure you have run all the setup cells (3-9) first.\n",
      "\n",
      "ğŸš€ Launching Enhanced Climate Dashboard on port 7860...\n",
      "ğŸ“Š Dashboard Features:\n",
      "   - Country Analysis with PySpark backend\n",
      "   - Global Comparisons\n",
      "   - AI Temperature Predictions (demo)\n",
      "   - System Monitoring\n",
      "   - Interactive Visualizations\n",
      "\n",
      "ğŸŒ Dashboard should be available at: http://localhost:7860\n",
      "ğŸ”— Public link will be generated...\n",
      "ğŸ’¡ Tips:\n",
      "   - Uncomment one of the launch commands above\n",
      "   - Or run: launch_dashboard() in a new cell\n",
      "   - For public sharing: launch_dashboard(share=True)\n",
      "   - For custom port: launch_dashboard(port=8080)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "* Running on public URL: https://1d07a3ab1b2d2c144c.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
      "* Running on public URL: https://1d07a3ab1b2d2c144c.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://1d07a3ab1b2d2c144c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# ğŸŒ Launch the Enhanced Climate Dashboard\n",
    "\n",
    "print(\"ğŸš€ Ready to launch the Enhanced Climate Analysis Dashboard!\")\n",
    "print(\"ğŸ“‹ Make sure you have run all the setup cells (3-9) first.\")\n",
    "print()\n",
    "\n",
    "# Uncomment the line below to launch the dashboard:\n",
    "launch_dashboard(share=True, port=7860)\n",
    "\n",
    "# Or run this for local access only:\n",
    "# launch_dashboard()\n",
    "\n",
    "print(\"ğŸ’¡ Tips:\")\n",
    "print(\"   - Uncomment one of the launch commands above\")\n",
    "print(\"   - Or run: launch_dashboard() in a new cell\")\n",
    "print(\"   - For public sharing: launch_dashboard(share=True)\")\n",
    "print(\"   - For custom port: launch_dashboard(port=8080)\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
